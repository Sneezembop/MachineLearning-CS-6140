{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Miles Benjamin - CS 6140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import pprint as pp\n",
    "import skimage as ski\n",
    "import skimage.transform as skit\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data is [10]\n",
      "shape of the data is [192, 168, 50]\n",
      "shape of the data is [10, 1]\n"
     ]
    }
   ],
   "source": [
    "mat = scio.loadmat('./ExtYaleB10.mat')\n",
    "#pp.pprint(mat)\n",
    "Y_test = mat['test']\n",
    "Y_train = mat['train']\n",
    "#print(Y_train[0])\n",
    "print('shape of the data is [%d]' % Y_train[0].shape)\n",
    "print('shape of the data is [%d, %d, %d]' % Y_train[0][0].shape)\n",
    "Y = np.mat(Y_train).T\n",
    "print('shape of the data is [%d, %d]' % Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the array of 50 images\n",
    "def downsampleImage(imgs):\n",
    "    img = []\n",
    "    temp = []\n",
    "    for i in range(len(imgs[0][0])):\n",
    "            img = imgs[:][:][i]\n",
    "            img = skit.resize(img,[20,17],mode='constant').flatten()\n",
    "            temp.append(img)\n",
    "            \n",
    "    #print(np.array(temp).shape)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = downsampleImage(Y_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(x):\n",
    "    if (x < 0):\n",
    "        x = 0\n",
    "    return x    \n",
    "def sig(x):\n",
    "    x = np.exp(x)\n",
    "    x += 1\n",
    "    x = 1/x\n",
    "    return x\n",
    "    \n",
    "def ht(x):\n",
    "    temp = np.exp(x) - np.exp(-x)\n",
    "    temp = temp /(np.exp(x) + np.exp(-x))\n",
    "    return temp\n",
    "\n",
    "def activationFunc(X, activ):\n",
    "    if (activ == 'rec'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = rec(X[i])\n",
    "            return X\n",
    "    elif (activ == 'sig'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = sig(X[i])\n",
    "            return X\n",
    "    elif (activ == 'ht'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = ht(X[i])\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recD(x):\n",
    "    if (x <= 0):\n",
    "        return 0\n",
    "    return 1\n",
    "def sigD(x):\n",
    "    y = sig(x) * (1- sig(x))\n",
    "    return y\n",
    "    \n",
    "def htD(x):\n",
    "    temp = 1- np.tanh(x)**2\n",
    "    return temp\n",
    "\n",
    "def activationFuncD(X, activ):\n",
    "    if (activ == 'rec'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = recD(X[i])\n",
    "            return X\n",
    "    elif (activ == 'sig'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = sigD(X[i])\n",
    "            return X\n",
    "    elif (activ == 'ht'):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = htD(X[i])\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0066927509923217993], [0.46211715726000979], [0.0066927509923217993], [0.46211715726000979], [0.0066927509923217993], [0.46211715726000979]]\n"
     ]
    }
   ],
   "source": [
    "temp = ['rec', 'sig', 'ht']\n",
    "temp2 = [[5], [-5]]\n",
    "\n",
    "temp3 = []\n",
    "for i in range(len(temp)):\n",
    "    for j in range(len(temp2)):\n",
    "        temp3.append(activationFunc(temp2[j], temp[i]))\n",
    "\n",
    "print(temp3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W's shape is as follows: W[l][s(l+1)][s(l)]\n",
    "\n",
    "def FFNN_Initialize(data, s1, s2, s3):\n",
    "    X = [0] * s1\n",
    "    X = np.mat(X)\n",
    "    Y = []\n",
    "    temp = 0\n",
    "    tempx = []\n",
    "    cat = len(data[0])\n",
    "    for i in range(len(data[0])):\n",
    "        tempx = downsampleImage(data[0][i])\n",
    "        tempx = np.mat(tempx)   \n",
    "        X = np.vstack((X,tempx))\n",
    "        temp = [i/cat] * len(tempx)\n",
    "        Y.append(temp)\n",
    "    \n",
    "    X = X[1:][:]\n",
    "    Y = np.array(Y).flatten()\n",
    "    \n",
    "    B = []\n",
    "    S = [s1,s2,s3,1]\n",
    "    W = [0] * 3\n",
    "    for i in range(len(W)):\n",
    "        W[i] = [0] * S[i+1]\n",
    "        for j in range(len(W[i])):\n",
    "            W[i][j] = [np.random.rand() * 0.0001] * S[i]\n",
    "       \n",
    "    for i in range(len(S) -2):\n",
    "        B.append(np.random.rand() * 0.0001)\n",
    "    \n",
    "    return X,Y,W,B,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFNN_FeedForward(xi,yi,W,B,S, activ):\n",
    "    L = (len(S) -1)\n",
    "    Z = [0] * L\n",
    "    A = [0] * L\n",
    "    \n",
    "    Z[0] = xi\n",
    "    A[0] = xi\n",
    "    lastA = xi\n",
    "    for i in range(L - 1):\n",
    "        temp = [0]*S[i+1]\n",
    "  \n",
    "        for j in range(S[i+1]):\n",
    "            temp[j] = np.mat(lastA) * np.mat(W[i][j]).T\n",
    "            temp[j] += B[i]\n",
    "            temp[j] = temp[j].A1.tolist()\n",
    "        \n",
    "        temp = np.mat(temp).T.A\n",
    "        \n",
    "        Z[i+1] = temp[0]\n",
    "        A[i+1] = activationFunc(temp[0], activ)\n",
    "       # if (i +1 == 2):\n",
    "           # print(temp, A[i +1], yi)\n",
    "        lastA = A[i+1]\n",
    "    \n",
    "    \n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFNN_BackProp(Z,A,yi,W,B,S, activ, ro):\n",
    "    L = len(S) -1\n",
    "    Delta = [0] * L\n",
    "    \n",
    "    Delta[L-1] = np.multiply((A[L-1] - yi),activationFuncD(np.mat(Z[L-1]).T.A, activ))\n",
    "   # print(Delta[L-1])\n",
    "    for i in range(L -2, -1, -1):\n",
    "        # print(np.array(Z[i]).shape)\n",
    "        Delta[i] = np.multiply(np.mat(W[i]).T * np.mat(Delta[i+1]),activationFuncD(np.mat(Z[i]).T.A, activ))\n",
    "    \n",
    "    #print(Delta)\n",
    "    for i in range(L -1):\n",
    "        W[i] = W[i] - ro * (Delta[i+1] * np.mat(A[i]))\n",
    "        B[i] = B[i] - ro * np.sum(Delta[i +1])\n",
    "    \n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activ takes the following values: sig, ht, rec\n",
    "def FFNN_Train(data, s1, s2, s3, activ, ro):\n",
    "    X,Y,W,B,S = FFNN_Initialize(data, s1, s2, s3)\n",
    "\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        Z,A = FFNN_FeedForward(X[i],Y[i],W,B,S, activ)\n",
    "        W,B = FFNN_BackProp(Z,A,Y[i],W,B,S, activ, ro)\n",
    "    \n",
    "    print(\"training complete\")\n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFNN_Test(data, s1, s2, s3, activ, W,B):\n",
    "    X,Y,temp,temp2,S = FFNN_Initialize(data,s1,s2,s3)\n",
    "\n",
    "    error = 0\n",
    "    print(\"testing to begin\")\n",
    "    for i in range(len(X)):\n",
    "        Z,A = FFNN_FeedForward(X[i],Y[i],W,B,S,activ)\n",
    "        error += np.abs(Y[i] - A[2])\n",
    "        #print('Z: ', Z[2],' A :', A[2], ' Y: ', Y[i])\n",
    "      \n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W's shape is as follows: W[l][s(l+1)][s(l)]\n",
    "\n",
    "def Auto_Initialize(data, s1, s2, s3):\n",
    "    X = [0] * s1\n",
    "    X = np.mat(X)\n",
    "    Y = []\n",
    "    temp = 0\n",
    "    tempx = []\n",
    "    cat = len(data[0])\n",
    "    for i in range(len(data[0])):\n",
    "        tempx = downsampleImage(data[0][i])\n",
    "        tempx = np.mat(tempx)   \n",
    "        X = np.vstack((X,tempx))\n",
    "\n",
    "    \n",
    "    X = X[1:][:]\n",
    "    Y = X\n",
    "    \n",
    "    B = []\n",
    "    S = [s1,s2,s3,1]\n",
    "    W = [0] * 3\n",
    "    for i in range(len(W)):\n",
    "        W[i] = [0] * S[i+1]\n",
    "        for j in range(len(W[i])):\n",
    "            W[i][j] = [np.random.rand() * 0.0001] * S[i]\n",
    "       \n",
    "    for i in range(len(S) -2):\n",
    "        B.append(np.random.rand() * 0.0001)\n",
    "    \n",
    "    return X,Y,W,B,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activ takes the following values: sig, ht, rec\n",
    "def Auto_Encode(data, s1, s2, s3, activ, ro):\n",
    "    X,Y,W,B,S = Auto_Initialize(data, s1, s2, s3)\n",
    "\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        Z,A = FFNN_FeedForward(X[i],Y[i],W,B,S, activ)\n",
    "        W,B = FFNN_BackProp(Z,A,Y[i],W,B,S, activ, ro)\n",
    "    \n",
    "    print(\"training complete\")\n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W,B = Auto_Encode(Y_train, 340, 3, 340, 'ht', 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Algorithms on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a. Testing NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "testing to begin\n",
      "[ 35.00000074]\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "\n",
    "# I'm pretty sure there's a bug in my code that's causing my NN to underperform.  \n",
    "# However I'm up against the time deadline and I need to keep going.\n",
    "\n",
    "# used to determine my best parameters takes forever to run, I don't recommend it\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ros = [1e-05,1e-06, 1e-07]\n",
    "s2s = [3,4,5,6,7]\n",
    "func = ['rec','sig','ht']\n",
    "bestRun = [0,0,0,1000000]\n",
    "\n",
    "for i in range(len(ros)):\n",
    "    for j in range(len(s2s)):\n",
    "        for k in range(len(func)):\n",
    "            print(\"ro: \", ros[i], \" s2: \", s2s[j], \" func: \", func[k])\n",
    "            W,B = FFNN_Train(Y_train, 340, s2s[j], 1, func[k], ros[i])\n",
    "            error = FFNN_Test(Y_test, 340, s2s[j], 1, func[k], W,B)\n",
    "            print(\"error: \", error)\n",
    "            if (error < bestRun[3]):\n",
    "                bestRun[0] = ros[i]\n",
    "                bestRun[1] = s2s[j]\n",
    "                bestRun[2] = func[k]\n",
    "                bestRun[3] = error\n",
    "\n",
    "print(\"Best Run:\", bestRun)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Best Run: [1e-05, 3, 'sig', array([ 35.00000065])]\n",
    "W,B = FFNN_Train(Y_train, 340, 3, 1, 'sig', 1e-05)\n",
    "error = FFNN_Test(Y_test, 340, 3, 1, 'sig', W,B)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. 1 v all SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Flattener(data,s):\n",
    "    X = [0] * s\n",
    "    X = np.mat(X)\n",
    "    Y = []\n",
    "    temp = 0\n",
    "    tempx = []\n",
    "    cat = len(data[0])\n",
    "    for i in range(len(data[0])):\n",
    "        tempx = downsampleImage(data[0][i])\n",
    "        tempx = np.mat(tempx)   \n",
    "        #print(tempx.shape)\n",
    "        X = np.vstack((X,tempx))\n",
    "        temp = [i] * len(tempx)\n",
    "        Y.append(temp)\n",
    "    \n",
    "    X = X[1:][:]\n",
    "    Y = np.array(Y).flatten()\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def kernelGen(X):\n",
    "    K = []\n",
    "    for i in range(len(X)):\n",
    "        temp = []\n",
    "        for j in range(len(X)):\n",
    "            xi = np.mat(X[i])\n",
    "            xj = np.mat(X[j])\n",
    "            \n",
    "            temp.append(np.dot(xi,xj.T).A)\n",
    "        K.append(np.concatenate(temp).ravel().tolist())\n",
    "\n",
    "    return K\n",
    "def dualF(Y,K,j,b,alpha):\n",
    "    E = 0\n",
    "    for i in range(len(K)):\n",
    "        innx = K[i][j]\n",
    "        \n",
    "        E += (alpha[i] * Y[i] * innx)\n",
    "        \n",
    "    E = E + b - Y[j]\n",
    "    \n",
    "    \n",
    "    return E\n",
    "def simplifiedSMO(K,Y,c,tol,maxPass):\n",
    "    alpha = np.zeros((len(K)))\n",
    "    b = 0\n",
    "    passes = 0\n",
    "    while(passes < maxPass):\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(len(K)):\n",
    "            Ei = dualF(Y, K, i, b, alpha)\n",
    "            temp = Ei * Y[i]\n",
    "            \n",
    "            if (((temp < (-1 * tol)) and (alpha[i] < c)) or ((temp > tol) and (alpha[i] > 0))):\n",
    "                j = random.randint(0, len(K) -1)\n",
    "                while (j == i):\n",
    "                    j = random.randint(0, len(K) -1)\n",
    "                    \n",
    "                Ej = dualF(Y, K, j, b, alpha)\n",
    "                # Save off old Alphas\n",
    "                alOldi = alpha[i]\n",
    "                alOldj = alpha[j]\n",
    "                \n",
    "                # Calculate L and H\n",
    "                L = max(0, alpha[j] - alpha[i])\n",
    "                H = min(c, c + alpha[j] - alpha[i])\n",
    "                #print(str(L) + \" \" + str(H))\n",
    "                \n",
    "                if (L == H):\n",
    "                    continue\n",
    "                #calculate n (eta)\n",
    "                \n",
    "                n = (2* K[i][j]) - (K[i][i]) - (K[j][j])\n",
    "                if (float(n) >= 0):\n",
    "                    continue\n",
    "                \n",
    "                #calculate new aj\n",
    "                alNewj = alOldj - ((Y[j] * (Ei - Ej))/n)\n",
    "                alNewj = np.clip(alNewj, 0, c)\n",
    "                \n",
    "                \n",
    "                if (alNewj > H):\n",
    "                    alNewj = H\n",
    "                elif (alNewj < L):\n",
    "                    alNewj = L\n",
    "                \n",
    "                if (abs(alNewj - alOldj) < 10**(-5)):\n",
    "                    continue\n",
    "                \n",
    "                alNewi = alOldi + (Y[i] * Y[j] * (alOldj - alNewj))\n",
    "                alNewi = np.clip(alNewi, 0, c)\n",
    "                \n",
    "                alpha[i] = alNewi\n",
    "                alpha[j] = alNewj\n",
    "                \n",
    "                # calculate bs\n",
    "                b1 = b - Ei - (Y[i] * (alpha[i] - alOldi) * K[i][i]) - (Y[j] * (alpha[j] - alOldj) * K[i][j])\n",
    "                b2 = b - Ej - (Y[i] * (alpha[i] - alOldi) * K[i][j]) - (Y[j] * (alpha[j] - alOldj) * K[j][j])\n",
    "                \n",
    "                b = (b1 + b2)/2\n",
    "                \n",
    "                if ((0 < alpha[i]) and (alpha[i] < c)):\n",
    "                    b = b1\n",
    "                elif ((0 < alpha[j]) and (alpha[i] < c)):\n",
    "                    b = b2\n",
    "                \n",
    "                num_changed_alphas += 1\n",
    "                \n",
    "                #w = CostW(K,Y,alpha)\n",
    "                #print(w)\n",
    "            #end if\n",
    "        #end for\n",
    "        if (num_changed_alphas == 0):\n",
    "            passes += 1\n",
    "        else:\n",
    "            passes = 0\n",
    "            \n",
    "    return [alpha, b]\n",
    "def errorFSVM(Y, Ycomp):\n",
    "    error = 0\n",
    "    for i in range(len(Y)):\n",
    "        if(Y[i] != Ycomp[i]):\n",
    "            error += 1\n",
    "\n",
    "    error = error / len(Y)  \n",
    "    return error\n",
    "def YPrep(Y, i):\n",
    "    Y_temp = [0] * len(Y)\n",
    "    for j in range(len(Y)):\n",
    "        if (Y[j] != i):\n",
    "            Y_temp[j] = -1\n",
    "        else:\n",
    "            Y_temp[j] = 1\n",
    "    return Y_temp  \n",
    "\n",
    "def SVMForAll(data, cat):\n",
    "    #30336\n",
    "    c = 0.5\n",
    "    tol = 10**(-5)\n",
    "    max_pass = 50\n",
    "    X, Y = Data_Flattener(data, 340)\n",
    "    X = np.mat(X)\n",
    "    K = kernelGen(X)\n",
    "    \n",
    "    F = [0] * cat\n",
    "    for i in range(cat):\n",
    "        Y_temp = YPrep(Y,i)\n",
    "        F[i] = simplifiedSMO(K,Y_temp,c, tol, max_pass)\n",
    "        print(\"Class:\", i, \"vs all\")\n",
    "        SVMPost(F[i], X, Y_temp)\n",
    "        \n",
    "    return F    \n",
    "\n",
    "def SVMPost(F, X, Y_temp):\n",
    "    alpha = F[0]\n",
    "    b = F[1]\n",
    "\n",
    "    Wstar = 0\n",
    "    for i in range(len(X)):\n",
    "        Wstar += alpha[i]*Y_temp[i]*X[i]\n",
    "    \n",
    "    w = np.mat(Wstar)\n",
    "    X = np.mat(X).T\n",
    "\n",
    "    bias = np.mean(Y_temp - (w * X))\n",
    "\n",
    "    Y = np.sign((w * X) + bias)\n",
    "\n",
    "    #plt.plot(X.T, Y.T, 'o')\n",
    "    #plt.plot(X.T, Y_trn, '.')\n",
    "    error = errorFSVM(Y.T, Y_temp)\n",
    "\n",
    "    print(\"training error: \", error * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 vs all\n",
      "training error:  46.42857142857143 %\n",
      "Class: 1 vs all\n",
      "training error:  37.857142857142854 %\n",
      "Class: 2 vs all\n",
      "training error:  40.714285714285715 %\n",
      "Class: 3 vs all\n",
      "training error:  37.857142857142854 %\n",
      "Class: 4 vs all\n",
      "training error:  30.714285714285715 %\n",
      "Class: 5 vs all\n",
      "training error:  45.0 %\n",
      "Class: 6 vs all\n",
      "training error:  42.857142857142854 %\n",
      "Class: 7 vs all\n",
      "training error:  35.0 %\n",
      "Class: 8 vs all\n",
      "training error:  41.42857142857143 %\n",
      "Class: 9 vs all\n",
      "training error:  42.857142857142854 %\n"
     ]
    }
   ],
   "source": [
    "F = SVMForAll(Y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c. 1 v all LogRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def H(x, w):\n",
    "    h =  x * w\n",
    "    h = 1 + np.exp(h)\n",
    "    h = 1/h\n",
    "    h = h.flatten()\n",
    "    return h[0][0]\n",
    "\n",
    "def gradientDescent(X, Y, w, alpha, ittr, batch):\n",
    "   # print(X, Y, w, alpha)\n",
    "    m = len(Y)\n",
    "    \n",
    "    for i in range(ittr):\n",
    "        batchIdx = math.floor((sp.rand(1) * m))\n",
    "        miniX = X[batchIdx]\n",
    "        miniY = Y[batchIdx]\n",
    "    \n",
    "        miniX = np.mat(miniX)\n",
    "        miniY = np.mat(miniY)\n",
    "        \n",
    "        temp = (miniY.T - H(miniX,w))\n",
    "        temp = temp * miniX\n",
    "        temp = temp * alpha\n",
    "        \n",
    "        # - 2 lambda w?\n",
    "        \n",
    "        w = w - temp.T\n",
    "        #print(H(miniX,w))\n",
    "    \n",
    "    return w\n",
    "\n",
    "def logRegress(X_trn, Y_trn, ittr, dim, lrnRate, batch):\n",
    "\n",
    "    X = np.mat(X_trn)\n",
    "    Y = np.mat(Y_trn).T\n",
    "    w = [0] * dim\n",
    "    \n",
    "    w = np.mat(w).T\n",
    "    \n",
    "    w = gradientDescent(X, Y, w, lrnRate, ittr, batch)\n",
    "    \n",
    "\n",
    "    return w\n",
    "\n",
    "def errorFLog(Y, Ycomp):\n",
    "    error = 0\n",
    "    for i in range(len(Y)):\n",
    "        if (Y[i] != Ycomp[i]):\n",
    "            error += 1\n",
    "    \n",
    "    error = error / len(Y)\n",
    "    return error \n",
    "\n",
    "def YPreplog(Y, i):\n",
    "    Y_temp = [0] * len(Y)\n",
    "    for j in range(len(Y)):\n",
    "        if (Y[j] != i):\n",
    "            Y_temp[j] = 0\n",
    "        else:\n",
    "            Y_temp[j] = 1\n",
    "    Y_temp = np.mat(Y_temp)\n",
    "    #print(Y_temp)\n",
    "    return Y_temp   \n",
    "\n",
    "def logRegressForAll(data, cat):\n",
    "    \n",
    "    X,Y = Data_Flattener(data,340)\n",
    "    ittr = 1000\n",
    "    lrnRate = 0.01\n",
    "    batch = 1\n",
    "    \n",
    "    W = [0] * cat\n",
    "    for i in range(cat):\n",
    "        Y_temp = YPreplog(Y,i)\n",
    "        W[i] = logRegress(X, Y_temp,ittr, 340,lrnRate, batch)\n",
    "        print(\"Class:\", i, \"vs all\")\n",
    "        logRegressPost(W[i],X,Y_temp)\n",
    "          \n",
    "    return W\n",
    "\n",
    "def logRegressPost(w,X,Y_temp):\n",
    "    Y_sol = []\n",
    "    for i in range(len(X)):\n",
    "        Y_sol.append(np.round(H(X[i], w)).A1[0])\n",
    "    \n",
    "    #print(Y_sol)\n",
    "    Y_sol = np.mat(Y_sol)\n",
    "    #print(\"W: \", w)\n",
    "\n",
    "    error = errorFLog(Y_temp.T, Y_sol.T)\n",
    "    print(\"Training error: \", error * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 1 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 2 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 3 vs all\n",
      "Training error:  3.5999999999999996 %\n",
      "Class: 4 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 5 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 6 vs all\n",
      "Training error:  2.6 %\n",
      "Class: 7 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 8 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 9 vs all\n",
      "Training error:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "W = logRegressForAll(Y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelGenPCA(X):\n",
    "    K = []\n",
    "    for i in range(len(X)):\n",
    "        temp = []\n",
    "        for j in range(len(X)):\n",
    "            xi = np.mat(X[i])\n",
    "            xj = np.mat(X[j])\n",
    "            \n",
    "            temp.append(np.dot(xi,xj.T).A)\n",
    "        K.append(np.concatenate(temp).ravel().tolist())\n",
    "\n",
    "    return K\n",
    "\n",
    "def KPCA(Y, d):\n",
    "    K = kernelGenPCA(Y)\n",
    "    #K = KTilda(K)\n",
    "    \n",
    "    K_eig = np.linalg.eig(K)\n",
    "    lam = K_eig[0]\n",
    "    W = K_eig[1]\n",
    "    #print('shape of the lambdas is [%d]' % lam.shape)\n",
    "    #print('shape of the W is [%d, %d]' % W.shape)\n",
    "    \n",
    "    for i in range(len(lam)):\n",
    "        W[i] *= 1/lam[i]\n",
    "    \n",
    "    W_tild = []\n",
    "    tempL = lam\n",
    "    tempW = np.array(W)\n",
    "    for i in range(d):\n",
    "        topLam = np.argmax(tempL)\n",
    "        W_tild.append(tempW[topLam])\n",
    "        \n",
    "        tempL = np.delete(tempL, topLam)\n",
    "        tempW = np.delete(tempW, topLam, 0)\n",
    "    \n",
    "    W_tild = np.mat(W_tild)\n",
    "    #print(W_tild)\n",
    "    X = W_tild * K\n",
    "    \n",
    "    return X.A\n",
    "\n",
    "def KPCAprep(data, d):\n",
    "    X,Y = Data_Flattener(data, 340)\n",
    "    X = KPCA(X, d)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAlogRegressForAll(X,Y, cat, d):\n",
    "\n",
    "    ittr = 1000\n",
    "    lrnRate = 0.01\n",
    "    batch = 1\n",
    "    \n",
    "    W = [0] * cat\n",
    "    for i in range(cat):\n",
    "        Y_temp = YPreplog(Y,i)\n",
    "        W[i] = logRegress(X, Y_temp,ittr, d,lrnRate, batch)\n",
    "        print(\"Class:\", i, \"vs all\")\n",
    "        logRegressPost(W[i],X,Y_temp)\n",
    "          \n",
    "    return W\n",
    "\n",
    "def PCASVMForAll(X,Y, cat):\n",
    "    #30336\n",
    "    c = 0.5\n",
    "    tol = 10**(-5)\n",
    "    max_pass = 50\n",
    "    X = np.mat(X)\n",
    "    K = kernelGen(X)\n",
    "    \n",
    "    F = [0] * cat\n",
    "    for i in range(cat):\n",
    "        Y_temp = YPrep(Y,i)\n",
    "        F[i] = simplifiedSMO(K,Y_temp,c, tol, max_pass)\n",
    "        print(\"Class:\", i, \"vs all\")\n",
    "        SVMPost(F[i], X, Y_temp)\n",
    "        \n",
    "    return F    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regress for All:\n",
      "Class: 0 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 1 vs all\n",
      "Training error:  2.8000000000000003 %\n",
      "Class: 2 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 3 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 4 vs all\n",
      "Training error:  1.0 %\n",
      "Class: 5 vs all\n",
      "Training error:  0.8 %\n",
      "Class: 6 vs all\n",
      "Training error:  6.4 %\n",
      "Class: 7 vs all\n",
      "Training error:  10.0 %\n",
      "Class: 8 vs all\n",
      "Training error:  0.0 %\n",
      "Class: 9 vs all\n",
      "Training error:  10.0 %\n",
      "SVM for All:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: complex() >= int()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4ab1ca4d16ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCAlogRegressForAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM for All:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCASVMForAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-943d0472f7e2>\u001b[0m in \u001b[0;36mPCASVMForAll\u001b[0;34m(X, Y, cat)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mY_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYPrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplifiedSMO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vs all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mSVMPost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5a6b63aadf51>\u001b[0m in \u001b[0;36msimplifiedSMO\u001b[0;34m(K, Y, c, tol, maxPass)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: complex() >= int()"
     ]
    }
   ],
   "source": [
    "X,Y = KPCAprep(Y_train, 100)\n",
    "print(\"Log Regress for All:\")\n",
    "W = PCAlogRegressForAll(X.T,Y,10, 100)\n",
    "print(\"SVM for All:\")\n",
    "F = PCASVMForAll(X,Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
